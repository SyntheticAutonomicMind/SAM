<!-- SPDX-License-Identifier: CC-BY-NC-4.0 -->
<!-- SPDX-FileCopyrightText: Copyright (c) 2025 Andrew Wyatt (Fewtarius) -->


# Streaming Conversational Architecture

## Overview

SAM implements a streaming conversational architecture developed through The Unbroken Method. This architecture ensures natural, contextual interactions where the LLM provides conversational feedback while executing tools, eliminating the need for hardcoded status messages.

The architecture was designed specifically for SAM to enable seamless tool integration within natural conversation flow, with all conversational context generated dynamically by the language model rather than hardcoded in the application.

## Core Architecture Pattern

### Streaming Conversational Model

SAM's architecture is built on the principle that the LLM should generate both conversational content and tool calls together, creating a natural flow where tools are integrated seamlessly into the conversation.

```json
{
  "content": "Let me search for that information in the codebase.",
  "tool_calls": [{"function": {"name": "file_operations", "arguments": {"operation": "search", ...}}}]
}
```

**Key Principles:**
- LLM generates both conversational content AND tool calls together
- No separate hardcoded status messages (no "ðŸ› Executing tools..." injections)
- Tools execute inline within the conversation loop
- Conversational context is provided naturally by the LLM, not hardcoded
- Architecture developed specifically for SAM using The Unbroken Method

### SAM Implementation

#### Current Streaming Flow
1. **User Input** â†’ SAM API receives message
2. **LLM Processing** â†’ AI provider generates response with tool calls
3. **Tool Execution** â†’ SAM executes tools and captures results
4. **LLM Continuation** â†’ SAM feeds tool results back to LLM for natural integration
5. **Streaming Response** â†’ Combined conversation + tool results streamed to client

#### Example SAM Response
```
User: "Search the codebase for the ConversationManager class"
SAM: "Let me search for that in the codebase.

[file_operations tool executes]

I found the ConversationManager class in Sources/ConversationEngine/ConversationManager.swift. 
It handles conversation state management and persistence. Would you like me to show you the 
implementation or search for something specific within it?"
```

## Tool Categories

SAM's tools fall into three categories:

### 1. User-Facing Work Tools
Tools that perform work on behalf of the user:
- **file_operations** - Read, write, search files
- **terminal_operations** - Execute commands, manage PTY sessions
- **web_operations** - Search web, scrape content, research topics
- **document_operations** - Import/create PDF, DOCX, PPTX
- **memory_operations** - Store and retrieve memories (RAG)
- **build_version_control** - Git operations, build tasks
- **search_index** - Search codebase
- **subagent** - Spawn subagents for complex tasks

### 2. Agent Planning Tools
Tools the AI uses to manage its own workflow:
- **todo_operations** - AI tracks its own tasks and progress
- **think** - Extended reasoning and planning

**Important:** The `todo_operations` tool is for **agent task management**, not user todo lists. The AI uses this tool to:
- Break down complex user requests into subtasks
- Track progress on multi-step workflows
- Mark tasks as in-progress/completed as it works
- Plan parallelizable work

### 3. Meta Tools
Tools for collaboration and session management:
- **user_collaboration** - Pause for user input/validation
- **increase_max_iterations** - Request more iterations for complex work
- **list_system_prompts** - View available system prompts
- **list_mini_prompts** - View available mini prompts
- **read_tool_result** - Re-read previous tool execution results

## Implementation Details

### SharedConversationService.swift
- **Clean Implementation**: No hardcoded emoji or status messages
- **LLM-Driven**: All conversational context generated by the language model
- **Tool Integration**: Seamless tool execution within conversation flow
- **Streaming Compatible**: Works with Server-Sent Events (SSE) protocol

### System Prompt Guidance
The system prompts encourage natural conversational tool usage:

```
When using tools, always provide conversational context about what you're doing. 
For example:
- "I'll search your memory for that information..."
- "Let me check the codebase for that function..."
- "I'll help you research that topic..."

Provide this context naturally as part of your response, not as separate status messages.

Use todo_operations to manage YOUR OWN workflow when handling complex multi-step tasks.
This helps you track progress and ensures nothing is forgotten.
```

## UI Considerations

### Chat Bubbles vs Continuous Flow
Traditional chat bubbles may not be optimal for streaming collaborative software because:

1. **Interruption**: Bubbles create visual separation that breaks flow
2. **Tool Integration**: Tool results need to integrate seamlessly into conversation
3. **Streaming**: Continuous content updates work better with flowing text
4. **Collaboration**: Less like "chat" and more like collaborative document editing

### Recommended UI Pattern
- **Continuous Conversation Flow**: Like a collaborative document or IDE assistant
- **Inline Tool Results**: Tool outputs integrated into conversation naturally
- **Streaming Text**: Text appears as it's generated, not in discrete bubbles
- **Contextual Actions**: Actions available inline where relevant
- **No Visual Interruption**: Avoid chat bubbles that break conversational flow
- **Focus on Content**: Let the conversation itself be the interface, not UI chrome

### Chat Bubble Analysis
Traditional chat bubbles may be counterproductive for SAM because:
- **Breaks Flow**: Creates visual separation between related thoughts and tool results
- **Emphasizes "Chat"**: SAM is more like a collaborative assistant than a chat bot  
- **Interrupts Streaming**: Bubbles suggest discrete messages, not continuous collaboration
- **Reduces Focus**: UI chrome distracts from the actual conversation content

**Recommendation**: Consider a continuous text interface where the conversation flows naturally without artificial visual boundaries, similar to how collaborative editing tools present information.

## Benefits

### User Experience
- **Natural Interaction**: Feels like talking to a knowledgeable assistant
- **Transparent Process**: User sees what's happening without technical noise
- **Seamless Flow**: No interruption between conversation and tool usage
- **Contextual Responses**: LLM provides relevant context for each action

### Technical Benefits
- **Maintainable**: No hardcoded messages to maintain
- **Flexible**: LLM can adapt context to situation
- **Extensible**: New tools automatically get appropriate conversational context
- **Robust**: LLM handles edge cases and provides appropriate responses

## Comparison: Before vs After

### Before (Hardcoded Messages)
```
User: "Find the file operations code"
SAM: "ðŸ› Executing tools..."
SAM: "Tool execution completed successfully!"
SAM: "Found the file."
```

### After (Conversational)
```
User: "Find the file operations code" 
SAM: "Let me search for that in the codebase. I found FileOperationsTool.swift in 
Sources/MCPFramework/Tools/. This file implements all file system operations including 
read, write, search, and edit. Would you like me to show you a specific part?"
```

## Example: Agent Using todo_operations

When the user gives a complex request, the AI uses `todo_operations` to manage its own workflow:

```
User: "Please audit all the documentation and fix any inaccuracies"

SAM: "I'll break this down into manageable tasks and work through them systematically.

[Calls todo_operations to create plan:
 1. Read all documentation files
 2. Verify against source code
 3. Document discrepancies
 4. Fix critical issues
 5. Fix minor issues
]

I've created a plan to:
1. Read all 33 documentation files
2. Verify each claim against actual code
3. Document discrepancies
4. Fix critical issues first, then minor ones

I'll start by reading through the documentation files...

[Marks task 1 as in-progress via todo_operations]
[Executes file_operations to read docs]
[Marks task 1 as completed via todo_operations]

I've read all the documentation. Now verifying against source code...
```

The user never sees the todo_operations tool calls - they just see natural progress updates. The AI uses the todo list internally to track its own work.

## Implementation Status

- **Hardcoded Messages Removed**: All emoji/status messages eliminated
- **LLM-Driven Responses**: Natural conversational context generated by LLM
- **Tool Integration**: Seamless tool execution within conversation flow
- **Streaming Compatible**: Works with SSE protocol
- **System Prompt Enhancement**: Implemented and encourages conversational tool usage
- **UI Evaluation**: Chat bubbles currently used, continuous flow recommended for future

## Architecture Benefits

### For Users
- Natural, flowing conversation
- Transparent about what the AI is doing
- No technical noise or status messages
- Work integrates seamlessly into conversation

### For Developers
- No hardcoded messages to maintain
- LLM handles all conversational context
- New tools automatically get appropriate treatment
- Extensible and maintainable architecture

### For the AI
- Can explain what it's doing naturally
- Uses todo_operations to track complex workflows
- Can request collaboration when needed
- Can manage its own iterations and planning

## See Also

- [Tool Execution Flow](flows/tool_execution_flow.md) - How tools are invoked and results returned
- [MCP Tools Specification](MCP_TOOLS_SPECIFICATION.md) - Complete tool reference
- [Message Flow](flows/message_flow.md) - End-to-end message processing
